# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGzh-W3_YA8IzLaiBLSa-fa1IxW4nRMP

# Importing all necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""# EDA

Data loading
"""

data = pd.read_csv('dummy_transaction_data(in).csv')

data['Order_Date'] = pd.to_datetime(data['Order_Date'], format='%d/%m/%Y')

"""Data summary"""

print(data.info())
print(data.describe())

"""Missing values"""

zeros_transactions = (data['Amount'] == 0).sum()
missing_values_total = data.isna().sum().sum()

print(f"Zero transactions: {zeros_transactions}")
print(f"Missing values: {missing_values_total}")

total_transactions = data.shape[0]
zero_transactions_count = data[data['Amount'] == 0].shape[0]
proportion_zeros = zero_transactions_count / total_transactions

print(f"Total transactions: {total_transactions}")
print(f"Proportion of zeros: {proportion_zeros:.2%}")

# Descriptive statistics
desc_with_zeros = data['Amount'].describe()
data_cleaned = data[data['Amount'] != 0]
desc_without_zeros = data_cleaned['Amount'].describe()
print(desc_with_zeros, desc_without_zeros)

# Mean transaction amount with zeros
mean_with_zeros = data['Amount'].mean()

# Mean transaction amount without zeros
mean_without_zeros = data_cleaned['Amount'].mean()

mean_with_zeros, mean_without_zeros

data_cleaned = data[data['Amount']!=0]

"""Visualizing customer type distribution through a barplot"""

counts_customer_type = data_cleaned['Customer_Type'].value_counts()

plt.bar(counts_customer_type.index, counts_customer_type.values, color='skyblue', edgecolor='black')
plt.xlabel('Type of Customer')
plt.ylabel('Frequency')
plt.title('Type of Customer Distribution')
plt.show()

"""Visualizing distributions of amount of transaction through a boxplot to easily identify outliers"""

# Plotting the boxplot to identify outliers
plt.figure(figsize=(10, 6))
plt.boxplot(data['Amount'].dropna(), vert=False)
plt.title('Boxplot of Amount')
plt.xlabel('Amount')
plt.grid(True)
plt.show()

"""Now visualizing in which category the outliers are most frequent"""

# Box plot of Amount by Customer_Type
plt.figure(figsize=(10, 6))
sns.boxplot(x='Customer_Type', y='Amount', data=data)
plt.title('Amount by Customer Type')
plt.xlabel('Customer Type')
plt.ylabel('Amount')
plt.grid(True)
plt.show()

"""Identifying trends over time using line plots"""

# Aggregate transaction_amount by month
data_monthly = data.set_index('Order_Date').resample('M')['Amount'].sum().reset_index()

# Line plot for monthly transaction amounts
plt.figure(figsize=(12, 6))
plt.plot(data_monthly['Order_Date'], data_monthly['Amount'], marker='o')
plt.title('Monthly Transaction Amount Over Time')
plt.xlabel('Date')
plt.ylabel('Total Transaction Amount')
plt.xticks(rotation=45)
plt.show()

"""# Feature Engineering

Remove negative values in transaction amount
"""

clean_data = data_cleaned[data_cleaned['Amount']>0]

"""First, add the following new features: 'Total_Spend', 'Average_Spend', 'Order_Frequency', 'Last_Purchase_Date' and 'First_Purchase_Date'"""

# Calculate total spend, average spend, frequency, recency, and tenure
current_date = clean_data['Order_Date'].max()  # Assuming latest order date as the reference

customer_data = data_cleaned.groupby('Customer_Number').agg(
    Total_spend=('Amount', 'sum'),
    Average_spend=('Amount', 'mean'),
    Order_frequency=('Order_Date', 'count'),
    Last_Purchase_date=('Order_Date', 'max'),
    First_Purchase_date=('Order_Date', 'min')
).reset_index()

"""Now, based on these new features, come up with more relevant features to calculate historical CLTV."""

customer_data['Recency'] = (current_date - customer_data['Last_Purchase_date']).dt.days
customer_data['Tenure'] = (customer_data['Last_Purchase_date'] - customer_data['First_Purchase_date']).dt.days

"""We can then calculate historical CLTV through this estimated formula: CLTV = Average Spend x Frequency x Customer Lifetime.

Where:
- Average Spend: the average amount spent per transaction
- Frequency: the number of transactions
- Customer Lifetime: the time duration (tenure) a customer has been active in years
"""

customer_data['Historical CLTV'] = customer_data['Average_spend'] * customer_data['Order_frequency'] * (customer_data['Tenure']/365)

"""# Data Preparation for Modeling"""

clean_data = data_cleaned.merge(customer_data, on='Customer_Number', how='left')

"""Encode relevant categorical values"""

print(clean_data.head())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
clean_data['Customer_Type'] = le.fit_transform(clean_data['Customer_Type'])
print(clean_data.head())

features = ['Total_spend', 'Average_spend', 'Order_frequency', 'Recency']
X = clean_data[features]
y = clean_data['Historical CLTV']

# Handling outliers
z_scores = np.abs((y - y.mean()) / y.std())
mask = z_scores < 3
X_no_outliers = X[mask]
y_no_outliers = y[mask]

X_no_outliers = pd.DataFrame(X_no_outliers, columns=features)
y_no_outliers = pd.Series(y_no_outliers)

X_train, X_test, y_train, y_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.2, random_state=42)

print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')
print(f'Shape of y_train: {y_train.shape}')
print(f'Shape of y_test: {y_test.shape}')

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Prediction of CLV"""

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train_scaled, y_train)

from sklearn.metrics import mean_squared_error

# Predicting CLV
y_pred = rf_model.predict(X_test_scaled)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error: {mae}')

# Assuming you are using a tree-based model like RandomForestRegressor
importances = rf_model.feature_importances_
feature_importance = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Define features and target variable
features = ['Total_spend']
X = clean_data[features]
y = clean_data['Historical CLTV']

# Handling outliers
z_scores = np.abs((y - y.mean()) / y.std())
mask = z_scores < 3
X_no_outliers = X[mask]
y_no_outliers = y[mask]

# Ensure X_no_outliers is a DataFrame and y_no_outliers is a Series
X_no_outliers = pd.DataFrame(X_no_outliers, columns=features)
y_no_outliers = pd.Series(y_no_outliers)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# Predict using the trained model
lr_pred = lr_model.predict(X_test_scaled)

# Evaluate the performance
lr_mse = mean_squared_error(y_test, lr_pred)
lr_mae = mean_absolute_error(y_test, lr_pred)
lr_r2 = r2_score(y_test, lr_pred)

print(f'Linear Regression MSE: {lr_mse}')
print(f'Linear Regression MAE: {lr_mae}')
print(f'Linear Regression R-squared: {lr_r2}')

# Plot predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, lr_pred, alpha=0.5)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()

# Plot residuals
residuals = y_test - lr_pred
plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=50, color='blue', alpha=0.7)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

print(f'Statistical Summary of Residuals:\n{residuals.describe()}')

# Calculate z-scores of residuals
residual_z_scores = np.abs((residuals - residuals.mean()) / residuals.std())
residual_mask = residual_z_scores < 3

# Apply this mask to both X_test and y_test to remove outliers
X_test_refined = X_test_scaled[residual_mask]
y_test_refined = y_test[residual_mask]

# Refit the model with refined test set
lr_pred_refined = lr_model.predict(X_test_refined)

# Evaluate the refined model
lr_mse_refined = mean_squared_error(y_test_refined, lr_pred_refined)
lr_mae_refined = mean_absolute_error(y_test_refined, lr_pred_refined)
lr_r2_refined = r2_score(y_test_refined, lr_pred_refined)

print(f'Refined Linear Regression MSE: {lr_mse_refined}')
print(f'Refined Linear Regression MAE: {lr_mae_refined}')
print(f'Refined Linear Regression R-squared: {lr_r2_refined}')

# Apply log transformation if the distribution is highly skewed
clean_data_no_outliers['Total_spend_log'] = np.log1p(clean_data_no_outliers['Total_spend'])
clean_data_no_outliers['Historical CLTV_log'] = np.log1p(clean_data_no_outliers['Historical CLTV'])

# Visualize the transformed data
plt.figure(figsize=(10, 6))
sns.histplot(clean_data_no_outliers['Historical CLTV_log'], bins=50, kde=True)
plt.title('Distribution of Log-Transformed Historical CLTV')
plt.xlabel('Log-Transformed Historical CLTV')
plt.ylabel('Frequency')
plt.show()

# Define features and target variable for the transformed data
features = ['Total_spend_log']
X = clean_data_no_outliers[features]
y = clean_data_no_outliers['Historical CLTV_log']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# Predict using the trained model
lr_pred = lr_model.predict(X_test_scaled)

# Evaluate the performance
lr_mse = mean_squared_error(y_test, lr_pred)
lr_mae = mean_absolute_error(y_test, lr_pred)
lr_r2 = r2_score(y_test, lr_pred)

print(f'Linear Regression MSE: {lr_mse}')
print(f'Linear Regression MAE: {lr_mae}')
print(f'Linear Regression R-squared: {lr_r2}')

# Plot predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, lr_pred, alpha=0.5)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()

# Plot residuals
residuals = y_test - lr_pred
plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=50, color='blue', alpha=0.7)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

print(f'Statistical Summary of Residuals:\n{residuals.describe()}')